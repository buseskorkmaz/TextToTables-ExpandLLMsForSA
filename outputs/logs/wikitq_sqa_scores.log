METEOR, ROUGE, BLEU, and BERTScore Evaluation Results:
Model: test_cl-flan-t5-xl-msr_sqa-True.txt
METEOR score: 0.016163132148462496
ROUGE scores: {'rouge1': 0.028812623275993293, 'rouge2': 0.008036786884363444, 'rougeL': 0.020624610535433657, 'rougeLsum': 0.020673172049894436}
BLEU score: 4.219171422845756
BERTScore: 0.783431878904017

Model: test_other-flan-t5-xl-msr_sqa-True.txt
METEOR score: 0.014705135970517504
ROUGE scores: {'rouge1': 0.026799994943122518, 'rouge2': 0.009220090891384196, 'rougeL': 0.0208152472893492, 'rougeLsum': 0.02077356584204192}
BLEU score: 22.30857686616157
BERTScore: 0.7847055451118903