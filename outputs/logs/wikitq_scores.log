METEOR, ROUGE, BLEU, and BERTScore Evaluation Results:
Model: test_cl-flan-t5-xl-wikitable-True.txt
METEOR score: 0.009772704330765339
ROUGE scores: {'rouge1': 0.02673599797979566, 'rouge2': 0.007275403236974041, 'rougeL': 0.019719083715976644, 'rougeLsum': 0.019741437685532326}
BLEU score: 44.41985005060449
BERTScore: 0.805919230832317

Model: test_other-flan-t5-xl-wikitable-True.txt
METEOR score: 0.008350734004031326
ROUGE scores: {'rouge1': 0.023976734806516662, 'rouge2': 0.007395403245540633, 'rougeL': 0.019051837873409548, 'rougeLsum': 0.01907494179721596}
BLEU score: 22.242469397936766
BERTScore: 0.8057647567965609