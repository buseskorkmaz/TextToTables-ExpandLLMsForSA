METEOR, ROUGE, BLEU, and BERTScore Evaluation Results:
Model: log-test_cl-Llama-2-7b-chat-hf-False.txt
METEOR score: 0.0769565793729982
ROUGE scores: {'rouge1': 0.06591324423013403, 'rouge2': 0.016990172022617815, 'rougeL': 0.040373237065133766, 'rougeLsum': 0.04045756901029873}
BLEU score: 10.11775206605948
BERTScore: 0.7027972340583801

Model: log-test_other-Llama-2-7b-chat-hf-False.txt
METEOR score: 0.07651558420473005
ROUGE scores: {'rouge1': 0.06534531061251318, 'rouge2': 0.01859456477239565, 'rougeL': 0.04212594122776214, 'rougeLsum': 0.04215982847091458}
BLEU score: 8.70251396295304
BERTScore: 0.7038462162017822