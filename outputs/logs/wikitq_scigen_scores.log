METEOR, ROUGE, BLEU, and BERTScore Evaluation Results:
Model: log-test_cl-wikitable-mrs_sqa-scigen-table-True.txt
METEOR score: 0.1400001902978397
ROUGE scores: {'rouge1': 0.23143426361913655, 'rouge2': 0.05353250258374491, 'rougeL': 0.16928471293959363, 'rougeLsum': 0.1693133617243778}
BLEU score: 54.648475850383996
BERTScore: 0.8461225628852844

Model: log-test_other-wikitable-mrs_sqa-scigen-table-True.txt
METEOR score: 0.134452319198399
ROUGE scores: {'rouge1': 0.2308800965112791, 'rouge2': 0.0575115341964056, 'rougeL': 0.17036487857240842, 'rougeLsum': 0.17070919225854922}
BLEU score: 34.842683945984476
BERTScore: 0.8469212651252747